\documentclass[12pt]{beamer}
\usetheme{Frankfurt}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
% \author{Martin Dobiasch}
%\author{Martin \textsc{Dobiasch} \& Eugen \textsc{Havasi} \& Peter \textsc{Rjabcsenko}}
\author{Martin Dobiasch \& Eugen Havasi \& Peter Rjabcsenko}
%\title{}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 
\begin{document}

\begin{frame}
\titlepage
\end{frame}

%\begin{frame}
%\tableofcontents
%\end{frame}

\begin{frame}{General Setup}
Python \& sklearn

\begin{block}{Algorithms}
\begin{itemize}
\item KNN
\item Decision Trees
\item Naive Bayes
\item SVM (SVR)
\end{itemize}
\end{block}

10 fold cross validation

Values reported are mean $\pm$ standard deviation

\end{frame}

\section{Communities}
\begin{frame}{Dataset}
Communities and Crime Data Set

Number of Instances: 1994

Number of Attributes: 128

Contains missing values

Task: Predict Violent Crimes per Population
\end{frame}

\begin{frame}{Initial view}
\includegraphics[height=\textheight,width=\textwidth,keepaspectratio]{plots/communities_orig.png}
\end{frame}

\begin{frame}{Preparations 1/2}
\begin{block}{Irrelevant}
state, county, community, communityname, fold

(from Dataset description)
\end{block}

\begin{block}{New Feature}
Recent Immigration: Split up 'recent', 'past 10', 'past 8', 'past 5' into '0 to 3', '3 to 5', '5 to 8', '8 to 10'
\end{block}
\end{frame}

\begin{frame}{Preparations 2/2}
\begin{block}{Side Notes}
''percentage of households with social security income in 1989''
correlates with ''percentage of population that is 65 and over in age''.

'percentage of people 16 and over who are employed in management or professional occupations'
correlates with ''percentage of people 25 and over with a bachelors degree or higher education''.
\end{block}

\begin{block}{After Cleaning}
Only 66 features left
\end{block}
\end{frame}

\begin{frame}{Final view}
\includegraphics[height=\textheight,width=\textwidth,keepaspectratio]{plots/communities_final.png}
\end{frame}

\begin{frame}{Best parameters}
\begin{block}{KNN}
neighbors: 39, p: 1, weights: uniform
\end{block}

\begin{block}{Decision Tree}
'splitter': 'best', 'criterion': 'friedman\_mse', 'max\_features': 'auto'
\end{block}

\begin{block}{SVR}
'gamma': 0.1, 'C': 1, 'kernel': 'rbf'
\end{block}
\end{frame}

\begin{frame}{Results}
\resizebox{\linewidth}{!}{\input{tables/communities.tex}}
%KNN ({'n_neighbors': 39, 'p': 1, 'weights': 'uniform'}, 0.14411585292632384)
%Tree({'splitter': 'best', 'criterion': 'friedman_mse', 'max_features': 'auto'}, %0.14542545934079396)
%gnb ({}, 0.20893417278547977)
%svm ({'gamma': 0.1, 'C': 1, 'kernel': 'rbf'}, 0.20459216628575627)
\end{frame}

\section{Stock}
\begin{frame}{Dataset}
performances of stock portfolios

Number of Instances: 315

Number of Attributes: 12

No missing values

Task: predict various performance metrics

4 periods of data $\rightarrow$ accumulated data
\end{frame}

\begin{frame}{Initial/Final view}
\includegraphics[height=\textheight,width=\textwidth,keepaspectratio]{plots/stock_final.png}
\end{frame}

\begin{frame}{Preprocessing \& Results}
Different encodings of floats (',' vs. '.')

Numbers with '\%' sign

\begin{block}{Good (MAE $< 0.05$)}
Annual Return, Excess Return, Systematic Risk
\end{block}

\begin{block}{Bad}
Total Risk, Abs. Win Rate, Rel. Win Rate
\end{block}
\end{frame}

\begin{frame}{Results}
\resizebox{\linewidth}{!}{\input{tables/stock.tex}}

%Annual Return
%({'p': 1, 'n_neighbors': 13, 'weights': 'distance'}, 0.017464444966712522)
%Best {'p': 1, 'n_neighbors': 13, 'weights': 'distance'}
%({'max_features': 'auto', 'splitter': 'best', 'criterion': 'friedman_mse'}, 0.017480511823369488)
%Best {'max_features': 'auto', 'splitter': 'best', 'criterion': 'friedman_mse'}
%({}, 0.045353596283597149)
%({'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}, 0.037521455065540152)
%Best {'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}
%Excess Return
%({'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}, 0.029410711493046936)
%Best {'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}
%({'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}, 0.027639007161707793)
%Best {'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}
%({}, 0.030747304462243566)
%({'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}, 0.024376446796960709)
%Best {'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}
%Systematic Risk
%({'p': 1, 'n_neighbors': 1, 'weights': 'distance'}, 0.024608917252478796)
%Best {'p': 1, 'n_neighbors': 1, 'weights': 'distance'}
%({'max_features': 'auto', 'splitter': 'best', 'criterion': 'friedman_mse'}, 0.031696405813183427)
%Best {'max_features': 'auto', 'splitter': 'best', 'criterion': 'friedman_mse'}
%({}, 0.05533186206973556)
%({'kernel': 'linear', 'gamma': 0.001, 'C': 10}, 0.055430927962504581)
%Best {'kernel': 'linear', 'gamma': 0.001, 'C': 10}
%Total Risk
%({'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}, 66.379581864771779)
%Best {'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}
%({'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}, 64.344454982589511)
%Best {'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}
%({}, 64.237512184944563)
%({'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}, 55.224220447355592)
%Best {'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}
%Abs. Win Rate
%({'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}, 51.047246411047887)
%Best {'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}
%({'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}, 49.83591746228614)
%Best {'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}
%({}, 49.776696369867437)
%({'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}, 44.192224269048054)
%Best {'kernel': 'sigmoid', 'gamma': 100, 'C': 10000}
%Rel. Win Rate
%({'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}, 41.478207993268704)
%Best {'p': 2, 'n_neighbors': 14, 'weights': 'uniform'}
%({'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}, 40.675461749113211)
%Best {'max_features': 'sqrt', 'splitter': 'random', 'criterion': 'mse'}
%({}, 40.6361006383042)
%({'kernel': 'rbf', 'gamma': 10, 'C': 1000}, 38.240834600476589)
%Best {'kernel': 'rbf', 'gamma': 10, 'C': 1000}

\end{frame}


\section{Auto}
\begin{frame}{Initial/Final view}
\includegraphics[height=\textheight,width=\textwidth,keepaspectratio]{plots/auto_final.png}
\end{frame}

\begin{frame}{Preprocessing}

\end{frame}

\begin{frame}{Best Parameters}
%({'weights': 'uniform', 'n_neighbors': 291, 'p': 2}, 4.1350573536016384)
%Best {'weights': 'uniform', 'n_neighbors': 291, 'p': 2}
%({'splitter': 'best', 'max_features': 'auto', 'criterion': 'friedman_mse'}, 4.1554836861671154)
%Best {'splitter': 'best', 'max_features': 'auto', 'criterion': 'friedman_mse'}
%({}, 4.4882746764655668)
\begin{block}{KNN}
neighbors: 9, p: 2, weights: distance
\end{block}

\begin{block}{Decision Tree}
'splitter': 'best', 'criterion': 'friedman\_mse', 'max\_features': 'auto'
\end{block}

\begin{block}{SVR}
'gamma': 0.1, 'C': 1, 'kernel': 'linear'
\end{block}
\end{frame}

\begin{frame}{Results}
\resizebox{\linewidth}{!}{\input{tables/auto.tex}}
\end{frame}

\section{KDD}
\begin{frame}{Initial view}
\includegraphics[height=\textheight,width=\textwidth,keepaspectratio]{plots/kdd_orig.png}
\end{frame}

\begin{frame}{Preprocessing}
Many correlating features

No real information about features given

Removed $\sim$ 70 features
\end{frame}

\begin{frame}{Final view}
\includegraphics[height=\textheight,width=\textwidth,keepaspectratio]{plots/kdd_final.png}
\end{frame}

\begin{frame}{Findings}
Imputing mean vs. removing cols with missing values: latter gives better results

\begin{block}{KNN}
neighbors: 291, p: 2, weights: uniform
\end{block}

\begin{block}{Decision Tree}
'splitter': 'best', 'criterion': 'friedman\_mse', 'max\_features': 'auto'
\end{block}

\begin{block}{SVR}
'gamma': 0.1, 'C': 1, 'kernel': 'rbf'
\end{block}
% results from removing:
% ({'n_neighbors': 291, 'p': 2, 'weights': 'uniform'}, 4.1350573536016384)
% ({'splitter': 'best', 'criterion': 'friedman_mse', 'max_features': 'auto'}, 4.1544598320715354)
% Results from imputing mean
% ({'n_neighbors': 291, 'p': 2, 'weights': 'uniform'}, 4.289451889295445)
% ({'splitter': 'best', 'criterion': 'friedman_mse', 'max_features': 'auto'}, 4.2979202497555358)
\end{frame}

\begin{frame}{Results}
\resizebox{\linewidth}{!}{\input{tables/kdd.tex}}
\end{frame}

\section{Summary}
\begin{frame}
\begin{block}{Runtimes}
Used different PCs for testing $\rightarrow$ no real runtime measurements
\end{block}

Search space too large

More imputation strategies?

No feature selection

No autosklearn
\end{frame}

\end{document}